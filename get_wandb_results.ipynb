{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get HP search runs and get their best validation score (and corresponding eval score, unused here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "MODE = [\"hpsearch2\", \"hpsearch3\", \"hpsearchrec\", \"full_run\", \"full_run_long\"][3]\n",
    "\n",
    "project_name = {\n",
    "    \"hpsearch2\": \"maranc/thesis-hpsearch\",\n",
    "    \"hpsearch3\": \"maranc/thesis-hpsearch3\",\n",
    "    \"hpsearchrec\": \"maranc/thesis-hpsearchrec\",\n",
    "    \"full_run\": \"maranc/thesis-results\",\n",
    "    \"full_run_long\": \"maranc/thesis-results-long\",\n",
    "}[MODE]\n",
    "\n",
    "# Project is specified by <entity/project-name>\n",
    "\n",
    "hyperparemeters = {\n",
    "    \"hpsearch2\": [\"learning_rate\", \"batch_size\", \"gamma\", \"train/clip_range\", \"gae_lambda\", \n",
    "                   \"ent_coef\", \"vf_coef\", \"n_layers\", \"n_epochs\", \"max_grad_norm\"],\n",
    "    \"hpsearch3\": [\"learning_rate\", \"batch_size\", \"gamma\", \"train/clip_range\", \"gae_lambda\",\n",
    "                     \"ent_coef\", \"vf_coef\", \"net_layers\", \"net_width\", \"n_epochs\", \"max_grad_norm\"],\n",
    "    \"hpsearchrec\": [\"learning_rate\", \"batch_size\", \"gamma\", \"train/clip_range\", \"gae_lambda\",\n",
    "                     \"ent_coef\", \"vf_coef\", \"net_layers\", \"net_width\", \"n_epochs\", \"max_grad_norm\"],\n",
    "    \"full_run\": [\"learning_rate\", \"batch_size\", \"gamma\", \"train/clip_range\", \"gae_lambda\",\n",
    "                     \"ent_coef\", \"vf_coef\", \"net_layers\", \"net_width\", \"n_epochs\", \"max_grad_norm\"],\n",
    "    \"full_run_long\": [\"learning_rate\", \"batch_size\", \"gamma\", \"train/clip_range\", \"gae_lambda\",\n",
    "                        \"ent_coef\", \"vf_coef\", \"net_layers\", \"net_width\", \"n_epochs\", \"max_grad_norm\"],\n",
    "}[MODE]\n",
    "exp_parameters = [\"mast_distancing\", \"privileged\", \"changing_wind\", \"dynamic_mode\"]\n",
    "general_parameters = ['name', 'run_id', 'eval_power', 'eval_power_std', \"n_steps\", 'validation_power', 'validation_power_argmax_pd', 'validation_power_argmax']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_runs = pd.DataFrame(columns=general_parameters + hyperparemeters + exp_parameters)\n",
    "\n",
    "runs = api.runs(project_name)\n",
    "print(f\"Found {len(runs)} runs\")\n",
    "for run in runs:\n",
    "    # add row to the dataframe\n",
    "    history = run.history()\n",
    "\n",
    "    if MODE == \"hpsearch3\":\n",
    "        if \"hp_search3\" not in run.name:\n",
    "            continue\n",
    "        if run.state != \"finished\":\n",
    "            continue\n",
    "    # elif MODE == \"hpsearch2\":\n",
    "    #     if \"hp_search2\" not in run.name:\n",
    "    #         continue\n",
    "    elif MODE == \"full_run\":\n",
    "        if \"hp_search\" in run.name:\n",
    "            continue\n",
    "        if run.state != \"finished\":\n",
    "            continue\n",
    "    \n",
    "    # Get performance parameters\n",
    "    if \"eval/validation_total_power\" in history.columns:  \n",
    "        validation_power = history[\"eval/validation_total_power\"].dropna()\n",
    "        # Best validation power, and corresponding timestep\n",
    "        idx = validation_power.idxmax()\n",
    "        eval_power = history[\"eval/total_power\"][idx]\n",
    "        eval_power_std = history[\"eval/total_power_std\"][idx]\n",
    "        validation_power_max = validation_power.max()\n",
    "        validation_power_argmax_pd = validation_power.idxmax()\n",
    "        validation_power_argmax = np.argmax(validation_power)\n",
    "        print(f\"Run {run.name} has eval power {eval_power}\")\n",
    "\n",
    "    hp_dict = {}\n",
    "    for k in hyperparemeters:\n",
    "        if k in run.config.keys():\n",
    "            hp_dict[k] = run.config[k]\n",
    "        else:\n",
    "            hp_dict[k] = run.summary[k]\n",
    "    exp_dict = {k: run.config[k] for k in exp_parameters}\n",
    "\n",
    "    general_dict = {\n",
    "        \"name\": run.name,\n",
    "        \"run_id\": run.id,\n",
    "        \"eval_power\": eval_power,\n",
    "        \"eval_power_std\": eval_power_std,\n",
    "        \"n_steps\": run.config[\"n_steps\"],\n",
    "        \"validation_power\": validation_power_max,\n",
    "        \"validation_power_argmax_pd\": validation_power_argmax_pd,\n",
    "        \"validation_power_argmax\": validation_power_argmax\n",
    "    }\n",
    "\n",
    "    dict_runs.loc[len(dict_runs)] = [*general_dict.values(), *hp_dict.values(), *exp_dict.values()]\n",
    "print(dict_runs)\n",
    "\n",
    "if MODE == \"full_run\" or MODE == \"full_run_long\":\n",
    "    dict_runs[\"filtered_name\"] = dict_runs[\"name\"].apply(lambda x: \"PPO\".join(x.split(\"PPO\")[:2]))\n",
    "    dict_runs[\"filtered_name\"] = dict_runs[\"filtered_name\"].apply(lambda x: \"_\".join(x.split(\"_\")[2:]))\n",
    "else:\n",
    "    dict_runs[\"filtered_name\"] = dict_runs[\"name\"].apply(lambda x: \"_\".join(x.split(\"_\")[3:]))\n",
    "dict_runs.to_csv(f\"data/eval/scores/{MODE}_wandb_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload from download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load from csv\n",
    "dict_runs = pd.read_csv(f\"data/eval/scores/{MODE}_wandb_data.csv\")\n",
    "exp_types = dict_runs[\"filtered_name\"].unique()\n",
    "exp_types = sorted(exp_types)\n",
    "print(f\"Detected {len(exp_types)} different experiment types\")\n",
    "for exp in exp_types: \n",
    "    l = len(dict_runs[dict_runs[\"filtered_name\"] == exp])\n",
    "    print(f\"({l}) {exp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results for each experiment (this is for final results and not hpsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp_df = pd.DataFrame(columns=[\"filtered_name\"] + exp_parameters + hyperparemeters)\n",
    "for exp in exp_types:\n",
    "    runs = dict_runs[dict_runs[\"filtered_name\"] == exp]\n",
    "    print(f\"Found {len(runs)} runs for {exp}\")\n",
    "    exp_params = runs.iloc[0][exp_parameters].to_dict()\n",
    "\n",
    "    scores = runs[\"eval_power\"].to_numpy()\n",
    "    scores_std = runs[\"eval_power_std\"].to_numpy()\n",
    "\n",
    "    # Save eval results, to allow preliminary analysis\n",
    "    filename = \"\"\n",
    "    # if 4wt in filtered name\n",
    "    if \"4wt\" in exp:\n",
    "        filename += \"4Symm\"\n",
    "    elif \"_nt8_\" in exp:\n",
    "        filename += \"8LHS\"\n",
    "    elif \"_nt16_\" in exp:\n",
    "        filename += \"16LHS\"\n",
    "    filename += \"_\"\n",
    "    if exp_params[\"dynamic_mode\"] == \"observation_points\":\n",
    "        filename += \"DynOP\"\n",
    "        if exp_params[\"privileged\"]:\n",
    "            # MDs are [25, 75, 125] and [100, 150, 200]\n",
    "            if exp_params[\"mast_distancing\"] in [25, 100]:\n",
    "                filename += \"_p0\"\n",
    "            elif exp_params[\"mast_distancing\"] in [75, 150]:\n",
    "                filename += \"_p1\"\n",
    "            else:\n",
    "                filename += \"_p2\"\n",
    "    elif exp_params[\"changing_wind\"] == True:\n",
    "        filename += \"CW\"\n",
    "    else:\n",
    "        filename += \"FW\"\n",
    "        \n",
    "    if \"recPPO\" in exp:\n",
    "        filename += \"_RecPPO\"\n",
    "    else:\n",
    "        filename += \"_PPO\"\n",
    "    filename += \"_total_powers\"\n",
    "    # Shape is (2) [reps], we want (reps, length)\n",
    "    np.save(f\"data/eval/scores/{filename}.npy\", scores)\n",
    "    # np.save(f\"data/eval/scores/{filename}_std.npy\", scores_std)\n",
    "    \n",
    "    print(f\"{exp} ({filename})\")\n",
    "    print(f\"Scores: {scores[:, None]} mean: {np.mean(scores)} +- {np.mean(scores_std)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute best HPs (best mean val) and save them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp_df = pd.DataFrame(columns=[\"filtered_name\"] + exp_parameters + hyperparemeters)\n",
    "for exp in exp_types:\n",
    "    runs = dict_runs[dict_runs[\"filtered_name\"] == exp]\n",
    "        \n",
    "    # get mean and std of validation power for each hyperparameter combination and count of runs\n",
    "    agg = {\"validation_power\": [\"mean\", \"std\", \"count\"], \"eval_power\": [\"mean\", \"std\"], \"eval_power_std\": [\"mean\", \"std\"]}\n",
    "    x = runs.groupby(hyperparemeters).agg(agg)\n",
    "    x.reset_index(inplace=True)\n",
    "\n",
    "    # print every hyperparameter combination and its average val power\n",
    "    # sorted by val power mean\n",
    "\n",
    "    # get row with max val power mean\n",
    "    row = x.loc[x[(\"validation_power\", \"mean\")].idxmax()]\n",
    "    # print val and eval power\n",
    "\n",
    "    best_hp = row[hyperparemeters].to_dict()\n",
    "    best_hp = {k[0]: v for k, v in best_hp.items()}\n",
    "    exp_params = runs.iloc[0][exp_parameters].to_dict()\n",
    "    \n",
    "    best_hp_df.loc[len(best_hp_df)] = {**{\"filtered_name\": exp}, **best_hp, **exp_params}\n",
    "\n",
    "    # Save eval results, to allow preliminary analysis\n",
    "    filename = \"\"\n",
    "    # if 4wt in filtered name\n",
    "    if \"4wt\" in exp:\n",
    "        filename += \"4Symm\"\n",
    "    elif \"_nt8_\" in exp:\n",
    "        filename += \"8LHS\"\n",
    "    elif \"_nt16_\" in exp:\n",
    "        filename += \"16LHS\"\n",
    "    filename += \"_\"\n",
    "    if exp_params[\"dynamic_mode\"] == \"observation_points\":\n",
    "        filename += \"DynOP\"\n",
    "        if exp_params[\"privileged\"]:\n",
    "            # MDs are [25, 75, 125] and [100, 150, 200]\n",
    "            if exp_params[\"mast_distancing\"] in [25, 100]:\n",
    "                filename += \"_p0\"\n",
    "            elif exp_params[\"mast_distancing\"] in [75, 150]:\n",
    "                filename += \"_p1\"\n",
    "            else:\n",
    "                filename += \"_p2\"\n",
    "    elif exp_params[\"changing_wind\"] == True:\n",
    "        filename += \"CW\"\n",
    "    else:\n",
    "        filename += \"FW\"\n",
    "        \n",
    "    if \"recPPO\" in exp:\n",
    "        filename += \"_RecPPO\"\n",
    "    else:\n",
    "        filename += \"_PPO\"\n",
    "    filename += \"_total_powers\"\n",
    "    score = np.array([[[row[('eval_power', 'mean')]]]])\n",
    "    # Shape is (1,1,1)\n",
    "    # We have both runs, we plot the std of the average eval power over the eval runs\n",
    "    np.save(f\"data/eval/scores/{filename}.npy\", score)\n",
    "    np.save(f\"data/eval/scores/{filename}_std.npy\", np.array([[[row[('eval_power', 'std')]]]]))\n",
    "    \n",
    "    print(f\"{exp} ({filename})\")\n",
    "    print(f\"Validation power: {row[('validation_power', 'mean')]} +- {row[('validation_power', 'std')]}\")\n",
    "    print(f\"Evaluation power: {row[('eval_power', 'mean')]} +- {row[('eval_power', 'std')]}\")\n",
    "    print(\"\")\n",
    "print(best_hp_df)\n",
    "best_hp_df.to_csv(f\"data/eval/scores/{MODE}_best_hp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "windfarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
